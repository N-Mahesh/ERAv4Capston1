# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PyzN3BVt9VBlR4yhg4GKoZ9JeP7-fi2m
"""

# If on Colab, uncomment to ensure latest torch/torchvision as needed
# !pip -q install torch torchvision tensorboard==2.17.1

import os, sys, math, time, json, random, shutil, pathlib, datetime
from types import SimpleNamespace

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.cuda import amp
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter

import torchvision
from torchvision import transforms
from torchvision.datasets import ImageFolder, FakeData
from torchvision.models import resnet50

print("PyTorch:", torch.__version__)
print("Torchvision:", torchvision.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Device name:", torch.cuda.get_device_name(0))

# Colab defaults (safe). For EC2 full run, flip SMOKE_TEST=False and set DATA_ROOT to your ImageNet path.
CFG = SimpleNamespace(
    # Modes
    SMOKE_TEST=False,                    # True = FakeData quick run; False = real dataset from DATA_ROOT
    NUM_WORKERS=4,                      # Tune for EC2 (often 8–16 per GPU)
    PIN_MEMORY=True,

    # Data (set for EC2 run)
    DATA_ROOT="/path/to/imagenet",      # e.g., "/mnt/imagenet" (must contain train/ and val/ subfolders when SMOKE_TEST=False)
    NUM_CLASSES=1000,                   # ImageNet-1k

    # Training
    EPOCHS=2 if True else 180,          # keep tiny for smoke test; set 180–200 for real training
    BATCH_SIZE=64,                      # adjust to GPU memory; on A100 40GB you can go much higher
    BASE_LR=0.256,                      # Linear scaling rule: 0.1 for batch_size=256 -> here scaled for per-GPU batch
    WARMUP_EPOCHS=5,
    WEIGHT_DECAY=1e-4,
    MOMENTUM=0.875,
    LABEL_SMOOTHING=0.1,

    # Checkpointing & logs
    OUT_DIR="./runs_resnet50_imagenet",
    SAVE_BEST=True,
    SAVE_LAST=True,
    TB_LOG=True,
    MARKDOWN_LOG=True,                  # writes epoch-by-epoch Markdown logs (submission requirement)
    MD_LOG_FILE="training_log.md",

    # Mixed precision
    USE_AMP=True,

    # DDP
    USE_DDP=False,                      # Colab single-GPU: False. On EC2 multi-GPU: True and launch with torchrun.
    WORLD_SIZE=1,                       # torchrun sets this; here for fallback
    RANK=0,                             # torchrun sets this; here for fallback
    DIST_BACKEND="nccl",
)

# Derive device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
os.makedirs(CFG.OUT_DIR, exist_ok=True)

# Seed for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed(42)

print("Output directory:", os.path.abspath(CFG.OUT_DIR))

def init_distributed(cfg):
    if not cfg.USE_DDP:
        return False
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        cfg.RANK = int(os.environ["RANK"])
        cfg.WORLD_SIZE = int(os.environ["WORLD_SIZE"])
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
    else:
        local_rank = 0

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend=cfg.DIST_BACKEND, init_method="env://")
    return True

def is_main_process(cfg):
    return (not cfg.USE_DDP) or (cfg.RANK % 1 == 0 and dist.get_rank() == 0)

# Standard ImageNet transforms
def get_transforms():
    train_tfms = transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
    ])
    val_tfms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
    ])
    return train_tfms, val_tfms

def build_dataloaders(cfg):
    train_tfms, val_tfms = get_transforms()

    if cfg.SMOKE_TEST:
        # FakeData with ImageNet-like shapes; tiny sizes for quick end-to-end validation
        train_set = FakeData(size=2048, image_size=(3,224,224), num_classes=cfg.NUM_CLASSES, transform=train_tfms)
        val_set   = FakeData(size=1000, image_size=(3,224,224), num_classes=cfg.NUM_CLASSES, transform=val_tfms)
    else:
        train_set = ImageFolder(os.path.join(cfg.DATA_ROOT, "train"), transform=train_tfms)
        val_set   = ImageFolder(os.path.join(cfg.DATA_ROOT, "val"),   transform=val_tfms)

    if cfg.USE_DDP:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set, shuffle=True)
        val_sampler   = torch.utils.data.distributed.DistributedSampler(val_set, shuffle=False)
        shuffle_train = False
    else:
        train_sampler, val_sampler = None, None
        shuffle_train = True

    train_loader = DataLoader(
        train_set, batch_size=cfg.BATCH_SIZE, shuffle=shuffle_train,
        num_workers=cfg.NUM_WORKERS, pin_memory=cfg.PIN_MEMORY, sampler=train_sampler, drop_last=True
    )
    val_loader = DataLoader(
        val_set, batch_size=cfg.BATCH_SIZE, shuffle=False,
        num_workers=cfg.NUM_WORKERS, pin_memory=cfg.PIN_MEMORY, sampler=val_sampler
    )
    return train_loader, val_loader

class LabelSmoothingCE(nn.Module):
    def __init__(self, eps=0.1):
        super().__init__()
        self.eps = eps
        self.log_softmax = nn.LogSoftmax(dim=-1)
    def forward(self, logits, target):
        n_classes = logits.size(-1)
        log_probs = self.log_softmax(logits)
        with torch.no_grad():
            true_dist = torch.zeros_like(log_probs)
            true_dist.fill_(self.eps / (n_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.eps)
        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))

def build_model(cfg):
    # From scratch: no pretrained weights
    model = resnet50(weights=None, num_classes=cfg.NUM_CLASSES)
    return model

def build_optimizer_and_sched(cfg, model, steps_per_epoch):
    optimizer = optim.SGD(
        model.parameters(),
        lr=cfg.BASE_LR,
        momentum=cfg.MOMENTUM,
        weight_decay=cfg.WEIGHT_DECAY,
        nesterov=True
    )

    # Warmup + Cosine decay across EPOCHS
    warmup_iters = cfg.WARMUP_EPOCHS * steps_per_epoch
    total_iters  = cfg.EPOCHS * steps_per_epoch

    def lr_lambda(current_iter):
        if current_iter < warmup_iters:
            return float(current_iter) / max(1, warmup_iters)
        # cosine from warmup_iters -> total_iters
        progress = (current_iter - warmup_iters) / max(1, total_iters - warmup_iters)
        return 0.5 * (1 + math.cos(math.pi * progress))

    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
    return optimizer, scheduler

@torch.no_grad()
def accuracy(output, target, topk=(1,5)):
    maxk = max(topk)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    res = []
    for k in topk:
        correct_k = correct[:k].reshape(-1).float().sum(0)
        res.append((correct_k * (100.0 / batch_size)).item())
    return res

def save_checkpoint(state, is_best, out_dir):
    last_path = os.path.join(out_dir, "checkpoint_last.pt")
    torch.save(state, last_path)
    if is_best:
        best_path = os.path.join(out_dir, "checkpoint_best.pt")
        shutil.copyfile(last_path, best_path)

def train_one_epoch(cfg, model, loader, criterion, optimizer, scaler, epoch, scheduler=None):
    model.train()
    running_loss = 0.0
    running_top1 = 0.0
    running_top5 = 0.0
    n_samples = 0

    for step, (images, targets) in enumerate(loader):
        images = images.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)

        if cfg.USE_AMP and device.type == "cuda":
            # ✅ Updated AMP API (torch.amp.autocast + torch.amp.GradScaler)
            from torch import amp as torch_amp
            with torch_amp.autocast('cuda'):
                outputs = model(images)
                loss = criterion(outputs, targets)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

        if scheduler is not None:
            scheduler.step()  # step per iteration

        with torch.no_grad():
            top1, top5 = accuracy(outputs, targets, topk=(1, 5))

        bs = images.size(0)
        running_loss += loss.item() * bs
        running_top1 += top1 * bs
        running_top5 += top5 * bs
        n_samples += bs

    return (
        running_loss / n_samples,
        running_top1 / n_samples,
        running_top5 / n_samples,
    )


@torch.no_grad()
def validate(cfg, model, loader, criterion):
    model.eval()
    running_loss = 0.0
    running_top1 = 0.0
    running_top5 = 0.0
    n_samples = 0

    for images, targets in loader:
        images = images.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)
        outputs = model(images)
        loss = criterion(outputs, targets)
        top1, top5 = accuracy(outputs, targets, topk=(1,5))
        bs = images.size(0)
        running_loss += loss.item() * bs
        running_top1 += top1 * bs
        running_top5 += top5 * bs
        n_samples += bs

    return (running_loss / n_samples,
            running_top1 / n_samples,
            running_top5 / n_samples)

def write_markdown_header(cfg):
    md_path = os.path.join(cfg.OUT_DIR, cfg.MD_LOG_FILE)
    if os.path.exists(md_path):
        return
    with open(md_path, "w") as f:
        f.write("# Training Log (Epoch-by-Epoch)\n\n")
        f.write("| Epoch | LR | Train Loss | Train Top1 | Train Top5 | Val Loss | Val Top1 | Val Top5 |\n")
        f.write("|------:|---:|-----------:|-----------:|-----------:|---------:|---------:|---------:|\n")

def append_markdown_epoch(cfg, epoch, lr, tr_loss, tr_t1, tr_t5, va_loss, va_t1, va_t5):
    md_path = os.path.join(cfg.OUT_DIR, cfg.MD_LOG_FILE)
    with open(md_path, "a") as f:
        f.write(f"| {epoch} | {lr:.6f} | {tr_loss:.4f} | {tr_t1:.2f} | {tr_t5:.2f} | {va_loss:.4f} | {va_t1:.2f} | {va_t5:.2f} |\n")

def main(cfg=CFG):
    ddp = init_distributed(cfg)

    # TensorBoard writer only on main process
    tb = SummaryWriter(log_dir=os.path.join(cfg.OUT_DIR, "tb")) if (cfg.TB_LOG and is_main_process(cfg)) else None
    if cfg.MARKDOWN_LOG and is_main_process(cfg):
        write_markdown_header(cfg)

    # Data
    train_loader, val_loader = build_dataloaders(cfg)
    steps_per_epoch = len(train_loader)

    # Model
    model = build_model(cfg).to(device)

    # DDP wrap if needed
    if cfg.USE_DDP:
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)

    # Loss, Opt, Sched
    criterion = LabelSmoothingCE(eps=cfg.LABEL_SMOOTHING)
    optimizer, scheduler = build_optimizer_and_sched(cfg, model, steps_per_epoch)

    scaler = amp.GradScaler(enabled=(cfg.USE_AMP and device.type == "cuda"))

    best_val_top1 = -1.0
    global_iter = 0

    start_time = time.time()
    for epoch in range(1, cfg.EPOCHS + 1):
        if cfg.USE_DDP:
            train_loader.sampler.set_epoch(epoch)

        train_loss, train_t1, train_t5 = train_one_epoch(
            cfg, model, train_loader, criterion, optimizer, scaler, epoch, scheduler=scheduler
        )
        val_loss, val_t1, val_t5 = validate(cfg, model, val_loader, criterion)

        if tb is not None:
            tb.add_scalar("train/loss", train_loss, epoch)
            tb.add_scalar("train/top1", train_t1, epoch)
            tb.add_scalar("train/top5", train_t5, epoch)
            tb.add_scalar("val/loss", val_loss, epoch)
            tb.add_scalar("val/top1", val_t1, epoch)
            tb.add_scalar("val/top5", val_t5, epoch)
            tb.add_scalar("lr", optimizer.param_groups[0]["lr"], epoch)

        if cfg.MARKDOWN_LOG and is_main_process(cfg):
            append_markdown_epoch(
                cfg, epoch, optimizer.param_groups[0]["lr"],
                train_loss, train_t1, train_t5, val_loss, val_t1, val_t5
            )

        is_best = val_t1 > best_val_top1
        best_val_top1 = max(best_val_top1, val_t1)

        if is_main_process(cfg) and (cfg.SAVE_LAST or (cfg.SAVE_BEST and is_best)):
            # If DDP, model may be wrapped
            save_state = {
                "epoch": epoch,
                "model_state": model.module.state_dict() if hasattr(model, "module") else model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scaler_state": scaler.state_dict() if scaler is not None else None,
                "best_val_top1": best_val_top1,
                "cfg": vars(cfg),
            }
            save_checkpoint(save_state, is_best=is_best, out_dir=cfg.OUT_DIR)

        msg = (f"Epoch {epoch}/{cfg.EPOCHS}  "
               f"Train: loss {train_loss:.4f} | top1 {train_t1:.2f} | top5 {train_t5:.2f}  ||  "
               f"Val: loss {val_loss:.4f} | top1 {val_t1:.2f} | top5 {val_t5:.2f}  "
               f"(best top1: {best_val_top1:.2f})")
        if is_main_process(cfg):
            print(msg)

    total_time = (time.time() - start_time) / 60
    if is_main_process(cfg):
        print(f"Done. Total time: {total_time:.1f} min. Best Val Top-1: {best_val_top1:.2f}")

    if tb is not None:
        tb.flush()
        tb.close()

    if cfg.USE_DDP:
        dist.barrier()
        dist.destroy_process_group()

# Kick it off
main(CFG)







